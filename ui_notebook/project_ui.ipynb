{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U83Mkk3nxks8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c63204ff-396d-4dad-e041-5ee8fdb5427c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.12.0-py3-none-any.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 21.8 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.23.0)\n",
            "Collecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[K     |████████████████████████████████| 278 kB 60.8 MB/s \n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.21.6)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 72.7 MB/s \n",
            "\u001b[?25hCollecting paramiko\n",
            "  Downloading paramiko-2.12.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 81.1 MB/s \n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.23.1-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Collecting starlette==0.22.0\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi->gradio) (4.1.1)\n",
            "Collecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.2-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 237 kB/s \n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.9.24)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.1-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.8 MB/s \n",
            "\u001b[?25h  Downloading httpcore-0.16.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.0 MB/s \n",
            "\u001b[?25h  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.6)\n",
            "Collecting cryptography>=2.5\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 70.5 MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 79.9 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 82.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4711 sha256=f8299da89f341bf570cff9fe3f1c798f7aace7921105e8e0acf843bfd6ebfb0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=76ada1c936e997fc1df8ea4ae1645a9736f19e05f5ec34a74759423d469c1c93\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, gradio\n",
            "Successfully installed anyio-3.6.2 bcrypt-4.0.1 cryptography-38.0.4 fastapi-0.88.0 ffmpy-0.3.0 gradio-3.12.0 h11-0.12.0 httpcore-0.15.0 httpx-0.23.1 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.2 mdurl-0.1.2 orjson-3.8.3 paramiko-2.12.0 pycryptodome-3.16.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.22.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 189 kB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 74.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 83.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 61.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=ad7bdf51ce21e16ebd95f4d6d5ebc09b333fd5cc464929896f2161b24c54008a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.11.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.25.0.tar.gz (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 2.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.1.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.5.2.221124-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Collecting types-pytz>=2022.1.1\n",
            "  Downloading types_pytz-2022.6.0.1-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=f3e11d7342500e4aeff0e3e9ea12933e1597a05f281b9e5d83d544e584825e8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/92/33/6f57c7aae0b16875267999a50570e81f15eecec577ebe05a2e\n",
            "Successfully built openai\n",
            "Installing collected packages: types-pytz, pandas-stubs, openai\n",
            "Successfully installed openai-0.25.0 pandas-stubs-1.5.2.221124 types-pytz-2022.6.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 32.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 76.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
            "\u001b[K     |████████████████████████████████| 295 kB 80.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Installing collected packages: fonttools, contourpy, matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed contourpy-1.0.6 fonttools-4.38.0 matplotlib-3.6.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install -U matplotlib\n",
        "!pip install gradio\n",
        "!pip install -U sentence-transformers\n",
        "!pip install openai\n",
        "## after upgradeing matplotlib need to restrat runtime in colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! rm -r ./DeCo\n",
        "\n",
        "# provide username and token to connect to git for model artifacts\n",
        "%env git_username=xuhetom\n",
        "%env git_passowrd=<git_token>\n",
        "! git clone https://${git_username}:${git_passowrd}@github.com/ece1786-2022/DeCo.git"
      ],
      "metadata": {
        "id": "skL30TTzxyux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b11a50-b1cf-4470-e02c-512533bcc778"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "rm: cannot remove './DeCo': No such file or directory\n",
            "env: git_username=xuhetom\n",
            "env: git_passowrd=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\n",
            "Cloning into 'DeCo'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 205 (delta 8), reused 7 (delta 3), pack-reused 152\u001b[K\n",
            "Receiving objects: 100% (205/205), 139.64 MiB | 15.98 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n",
            "Checking out files: 100% (73/73), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import openai\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "import gradio"
      ],
      "metadata": {
        "id": "IZ_zqoiWybdO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting seed for all randomness\n",
        "def set_seed():\n",
        "  np.random.seed(1786)\n",
        "  torch.manual_seed(1786)\n",
        "  random.seed(1786)\n",
        "set_seed()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Baseline model classification head with 1 layer\n",
        "class Industry_Group_Classification_Baseline_Model(torch.nn.Module):\n",
        "  def __init__(self, emb_length=768, pred_length=24):\n",
        "      # Single Layer\n",
        "      super().__init__()\n",
        "      self.layer1 = nn.Linear(emb_length, pred_length)\n",
        "      \n",
        "  def forward(self, x_emb):\n",
        "    x_emb = torch.as_tensor(x_emb, dtype=torch.float)\n",
        "    if torch.cuda.is_available(): x_emb = x_emb.cuda()\n",
        "    \n",
        "    prediction = F.leaky_relu(self.layer1(x_emb))\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Target Model classification head for both BERT and GPT-3 with 4 layers\n",
        "class Industry_Group_Classification_Model(torch.nn.Module):\n",
        "  def __init__(self, emb_length=768, pred_length=24):\n",
        "      super().__init__()\n",
        "      self.layer1 = nn.Linear(emb_length, int(emb_length/2))\n",
        "      self.layer2 = nn.Linear(int(emb_length/2), int(emb_length/4))\n",
        "      self.layer3 = nn.Linear(int(emb_length/4), int(emb_length/12))\n",
        "      self.layer4 = nn.Linear(int(emb_length/12), pred_length)\n",
        "      # self.drop_out = nn.Dropout(p=0.2)\n",
        "      # self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=freeze)\n",
        "      # self.conv1 = nn.Conv2d(1, n_1, kernel_size = [k_1,embedding_size], bias=False)\n",
        "      # self.conv2 = nn.Conv2d(1, n_2, kernel_size = [k_2,embedding_size], bias=False)\n",
        "      # self.layer1 = nn.Linear(n_1+n_2, 1)\n",
        "      \n",
        "  def forward(self, x_emb):\n",
        "    x_emb = torch.as_tensor(x_emb, dtype=torch.float)\n",
        "    if torch.cuda.is_available(): x_emb = x_emb.cuda()\n",
        "    \n",
        "    out1 = F.leaky_relu(self.layer1(x_emb))\n",
        "    out2 = F.leaky_relu(self.layer2(out1))\n",
        "    out3 = F.leaky_relu(self.layer3(out2))\n",
        "    prediction = self.layer4(out3)\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Load model from saved files\n",
        "# Baseline model\n",
        "baseline_model_path = \"/content/DeCo/phase2/saved_model_weights/bert_baseline-emb_length=768-pred_length=24-batch_size=32-epochs=3000-lr=0.005-cut_off=0.3.pt\"\n",
        "classification_baseline_model_bert = Industry_Group_Classification_Baseline_Model(emb_length=768, pred_length=24)\n",
        "classification_baseline_model_bert.load_state_dict(torch.load(baseline_model_path))\n",
        "classification_baseline_model_bert.eval()\n",
        "summary(classification_baseline_model_bert.to(device), (1, 768))\n",
        "\n",
        "# Target 1 - BERT\n",
        "target_1_bert_model_path = \"/content/DeCo/phase2/saved_model_weights/Phase2_2_bert.pt\"\n",
        "classification_target_1_bert = Industry_Group_Classification_Model(emb_length=768, pred_length=24)\n",
        "classification_target_1_bert.load_state_dict(torch.load(target_1_bert_model_path))\n",
        "classification_target_1_bert.eval()\n",
        "summary(classification_target_1_bert.to(device), (1, 768))\n",
        "\n",
        "# Target 2 - GPT3-Ada\n",
        "target_2_gpt3_model_path = \"/content/DeCo/phase2/saved_model_weights/Phase2_2_gpt3.pt\"\n",
        "classification_target_2_gpt3 = Industry_Group_Classification_Model(emb_length=1024, pred_length=24)\n",
        "classification_target_2_gpt3.load_state_dict(torch.load(target_2_gpt3_model_path))\n",
        "classification_target_2_gpt3.eval()\n",
        "summary(classification_target_2_gpt3.to(device), (1, 1024))\n",
        "\n"
      ],
      "metadata": {
        "id": "cFIOZhTIyPNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63dd4cff-69a5-4ea2-f919-e2a322ebbcd2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                [-1, 1, 24]          18,456\n",
            "================================================================\n",
            "Total params: 18,456\n",
            "Trainable params: 18,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 0.07\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 1, 384]         295,296\n",
            "            Linear-2               [-1, 1, 192]          73,920\n",
            "            Linear-3                [-1, 1, 64]          12,352\n",
            "            Linear-4                [-1, 1, 24]           1,560\n",
            "================================================================\n",
            "Total params: 383,128\n",
            "Trainable params: 383,128\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 1.46\n",
            "Estimated Total Size (MB): 1.47\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 1, 512]         524,800\n",
            "            Linear-2               [-1, 1, 256]         131,328\n",
            "            Linear-3                [-1, 1, 85]          21,845\n",
            "            Linear-4                [-1, 1, 24]           2,064\n",
            "================================================================\n",
            "Total params: 680,037\n",
            "Trainable params: 680,037\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 2.59\n",
            "Estimated Total Size (MB): 2.60\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence-BERT Multilable classification inference\n",
        "\n",
        "label_ls = ['Automobiles & Components','Banks','Capital Goods',\n",
        " 'Commercial & Professional Services','Consumer Durables & Apparel',\n",
        " 'Consumer Services','Diversified Financials','Energy',\n",
        " 'Food & Staples Retailing','Food, Beverage & Tobacco',\n",
        " 'Health Care Equipment & Services','Household & Personal Products',\n",
        " 'Insurance','Materials','Media & Entertainment',\n",
        " 'Pharmaceuticals, Biotechnology & Life Sciences','Real Estate',\n",
        " 'Retailing','Semiconductors & Semiconductor Equipment',\n",
        " 'Software & Services','Technology Hardware & Equipment',\n",
        " 'Telecommunication Services','Transportation','Utilities']\n",
        "\n",
        "pretrained_model_bert = None\n",
        "def get_embedding_from_bert(texts, model_name='sentence-transformers/all-mpnet-base-v2'):\n",
        "  \"\"\"\n",
        "  Initialize sentence bert and extract embedding from the pre-trained model\n",
        "  \"\"\"\n",
        "  global pretrained_model_bert\n",
        "  if pretrained_model_bert == None: \n",
        "    pretrained_model_bert = SentenceTransformer(model_name)\n",
        "    print(\"BERT Initalized...\")\n",
        "  texts_embedding = pretrained_model_bert.encode(texts)\n",
        "  return texts_embedding\n",
        "\n",
        "def get_embedding_from_gpt3(texts, model_name=\"text-similarity-ada-001\"):\n",
        "  \"\"\"\n",
        "  Connect to gpt-3 api and get embedding of the given text\n",
        "  \"\"\"\n",
        "  texts_embedding = []\n",
        "  texts = texts.replace(\"\\n\", \" \")\n",
        "  texts_embedding = openai.Embedding.create(input = texts, model=model_name)['data'][0]['embedding']\n",
        "  return texts_embedding\n",
        "\n",
        "def model_inference(embeded_sentence, model):\n",
        "  \"\"\"\n",
        "  running inference to get prediction on a given model\n",
        "  \"\"\"\n",
        "  pred = torch.sigmoid(model(embeded_sentence))\n",
        "  pred_out = {}\n",
        "  for label, pred in zip(label_ls, pred.cpu().detach().numpy()):\n",
        "    pred_out[label] = pred\n",
        "  sorted_result = sorted([(k, v) for k, v in pred_out.items()], key=lambda x : x[1], reverse=True)\n",
        "  return sorted_result\n",
        "\n",
        "def plot_result(result, cut_off=0.3):\n",
        "  \"\"\"\n",
        "  return bar chart plot figure (to be used by the gradio ui). Cut_off to control colouring of the bar chart\n",
        "  \"\"\"\n",
        "  subplotsize=[6.,9.]\n",
        "  figuresize=[12.,10.]   \n",
        "\n",
        "  left = 0.5*(1.-subplotsize[0]/figuresize[0])\n",
        "  right = 1.-left\n",
        "  bottom = 0.5*(1.-subplotsize[1]/figuresize[1])\n",
        "  top = 1.-bottom\n",
        "\n",
        "  plt.style.use('dark_background')\n",
        "  fig = plt.figure(figsize=figuresize)\n",
        "  fig.subplots_adjust(left=left,right=right,bottom=bottom,top=top)\n",
        "  ax = fig.add_subplot(111)\n",
        "  label = [row[0] for row in result]\n",
        "  signal = [row[1] for row in result]\n",
        "  colours = ['grey' if (x < cut_off) else 'green' for x in signal ]\n",
        "  bar = ax.barh(label, signal, color=colours)\n",
        "  ax.bar_label(bar, fmt='%.3f')\n",
        "  ax.invert_yaxis() \n",
        "  # ax.tick_params(axis='x', colors='white')\n",
        "  # ax.tick_params(axis='y', colors='white')\n",
        "  return fig\n"
      ],
      "metadata": {
        "id": "XC449uAi_o1f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warm up both APIs\n",
        "sentence = \"\"\"\n",
        "Air Products and Chemicals, Inc., a Delaware corporation originally founded in 1940, serves customers globally with a unique portfolio of products, services, and solutions that include atmospheric gases, process and specialty gases, equipment, and services. Focused on serving energy, environment and emerging markets, we provide essential industrial gases, related equipment, and applications expertise to customers in dozens of industries, including refining, chemicals, metals, electronics, manufacturing, and food and beverage. We are the world‚Äôs largest supplier of hydrogen and have built leading positions in growth markets such as helium and liquefied natural gas (\"LNG\") process technology and equipment. We develop, engineer, build, own, and operate some of the world‚Äôs largest industrial gas projects, including gasification projects that sustainably convert abundant natural resources into syngas for the production of high-value power, fuels, and chemicals and are developing carbon capture projects and world-scale low carbon and carbon-free hydrogen projects that will support global transportation and energy transition away from fossil fuels. Our Industrial Gases business produces atmospheric gases, such as oxygen, nitrogen, and argon; process gases, such as hydrogen, helium, carbon dioxide (CO ), carbon monoxide, and syngas; and specialty gases. Atmospheric gases are produced through various air separation processes, of which cryogenic is the most prevalent. Process gases are produced by methods other than air separation. For example, hydrogen, carbon monoxide, and syngas are produced by steam methane reforming of natural gas and by the gasification of liquid and solid hydrocarbons. Hydrogen is produced by purifying byproduct sources obtained from the chemical and petrochemical industries. Helium is produced as a byproduct of gases extracted from underground reservoirs, primarily natural gas, but also CO purified before resale. The Industrial Gases business also develops, builds, and operates equipment for the production or processing of gases, such as air separation units and non-cryogenic generators.\n",
        "\"\"\"\n",
        "\n",
        "bert_embedding = get_embedding_from_bert(sentence)\n",
        "gpt3_embedding = get_embedding_from_gpt3(sentence)\n",
        "\n",
        "# # plot\n",
        "# baseline_result = model_inference(bert_embedding, classification_baseline_model_bert)\n",
        "# target_1_bert_result = model_inference(bert_embedding, classification_target_1_bert)\n",
        "# target_2_gpt3_result = model_inference(gpt3_embedding, classification_target_2_gpt3)\n",
        "\n",
        "# plot_baseline = plot_result(baseline_result)\n",
        "# plot_target_1_bert = plot_result(target_1_bert_result)\n",
        "# plot_target_2_gpt3 = plot_result(target_2_gpt3_result)"
      ],
      "metadata": {
        "id": "xHY5oVzqBfbU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.multiarray import min_scalar_type\n",
        "def prediction(sentence, \n",
        "               threshold_1, \n",
        "               threshold_2,\n",
        "               threshold_3,\n",
        "               baseline=classification_baseline_model_bert, \n",
        "               target_1_bert=classification_target_1_bert, \n",
        "               target_2_gpt3=classification_target_2_gpt3):\n",
        "  \n",
        "    print(threshold_1, threshold_2, threshold_3)\n",
        "\n",
        "    # embeddings\n",
        "    bert_embedding = get_embedding_from_bert(sentence)\n",
        "    gpt3_embedding = get_embedding_from_gpt3(sentence)\n",
        "\n",
        "    # plot\n",
        "    baseline_result = model_inference(bert_embedding, classification_baseline_model_bert)\n",
        "    target_1_bert_result = model_inference(bert_embedding, classification_target_1_bert)\n",
        "    target_2_gpt3_result = model_inference(gpt3_embedding, classification_target_2_gpt3)\n",
        "\n",
        "    plot_baseline = plot_result(baseline_result, cut_off=threshold_1)\n",
        "    plot_target_1_bert = plot_result(target_1_bert_result, cut_off=threshold_2)\n",
        "    plot_target_2_gpt3 = plot_result(target_2_gpt3_result, cut_off=threshold_3)\n",
        "\n",
        "    # return \"\\n\".join([f\"{row[0]}\\t {row[1]:.4f}\" for row in result]), plot\n",
        "    return plot_baseline, plot_target_1_bert, plot_target_2_gpt3\n",
        "\n",
        "ui = gradio.Blocks()\n",
        "\n",
        "with ui:\n",
        "    gradio.Markdown(\"DeCo. - Company Industry Group Decomposition Analyzer\")\n",
        "    input_text = gradio.Textbox(lines=10, placeholder=\"Company Description...\", label=\"Text to Analyze\")\n",
        "    btn = gradio.Button(value=\"Analyze\")\n",
        "    with gradio.Row():\n",
        "\n",
        "      # with gradio.Column(scale=1.3, min_width=700):\n",
        "      with gradio.Column():\n",
        "        gradio.Markdown(\"Baseline - BERT w/ Single Layer MLP Classification Head\")\n",
        "        threshold_1 = gradio.Slider(0, 1, value=0.3, label=\"Threshold\")\n",
        "        output1 = gradio.Plot(label=f\"Company Industry Group Decomposition\")\n",
        "      with gradio.Column():\n",
        "        gradio.Markdown(\"Target 1 - BERT w/ 4 Layer MLP Classification Head\")\n",
        "        threshold_2 = gradio.Slider(0, 1, value=0.5, label=\"Threshold\")\n",
        "        output2 = gradio.Plot(label=f\"Company Industry Group Decomposition\")\n",
        "      with gradio.Column():\n",
        "        gradio.Markdown(\"Target 1 - GPT-3 Ada w/ 4 Layer MLP Classification Head\")\n",
        "        threshold_3 = gradio.Slider(0, 1, value=0.5, label=\"Threshold\")\n",
        "        output3 = gradio.Plot(label=f\"Company Industry Group Decomposition\")\n",
        "\n",
        "    btn.click(prediction, [input_text, threshold_1, threshold_2, threshold_3],\n",
        "              [output1, output2, output3])\n",
        "\n",
        "ui.launch(share=True)\n"
      ],
      "metadata": {
        "id": "ZBWfOtFMyIJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "a31a8353-b0bb-4661-aaa4-46811e388c98"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://a1497372407237ba.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a1497372407237ba.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}